\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1}{Doc-Start}}
\@writefile{toc}{\contentsline {paragraph}{Linear Algebra Libraries}{2}{Doc-Start}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Written Questions [30 points]}{3}{section.1}}
\newlabel{sec:written}{{1}{3}{Written Questions [30 points]}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Perceptron and Stochastic Gradient Descent [4 points]}{3}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Multinomial Logistic Regression [13 points]}{4}{subsection.1.2}}
\newlabel{eq:1}{{1.1}{4}{Multinomial Logistic Regression [13 points]}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Binary Logistic Regression on a Small Dataset [5 points]}{10}{subsection.1.3}}
\newlabel{sec:warm-up}{{1.3}{10}{Binary Logistic Regression on a Small Dataset [5 points]}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Programming Empirical Questions [8 points]}{14}{subsection.1.4}}
\newlabel{sec:empirical}{{1.4}{14}{Programming Empirical Questions [8 points]}{subsection.1.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces ``Large Data'' Results\relax }}{16}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{results}{{1.1}{16}{``Large Data'' Results\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Programming [70 points]}{18}{section.2}}
\newlabel{programming}{{2}{18}{Programming [70 points]}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Task}{18}{subsection.2.1}}
\newlabel{task}{{2.1}{18}{The Task}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Datasets}{18}{subsection.2.2}}
\newlabel{dataset}{{2.2}{18}{The Datasets}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Model Definition}{19}{subsection.2.3}}
\newlabel{modeldescript}{{2.3}{19}{Model Definition}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Implementation}{20}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Overview}{20}{subsubsection.2.4.1}}
\newlabel{overview}{{2.4.1}{20}{Overview}{subsubsection.2.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Programming pipeline for sentiment analyzer based on binary logistic regression\relax }}{20}{figure.caption.3}}
\newlabel{pipeline}{{2.1}{20}{Programming pipeline for sentiment analyzer based on binary logistic regression\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Feature Engineering}{20}{subsubsection.2.4.2}}
\newlabel{feature}{{2.4.2}{20}{Feature Engineering}{subsubsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Command Line Arguments}{21}{subsubsection.2.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Program Outputs}{23}{subsection.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Output: Formatted Data Files}{23}{subsubsection.2.5.1}}
\newlabel{format_output}{{2.5.1}{23}{Output: Formatted Data Files}{subsubsection.2.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Output: Labels Files}{23}{subsubsection.2.5.2}}
\newlabel{output}{{2.5.2}{23}{Output: Labels Files}{subsubsection.2.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Output Metrics}{23}{subsubsection.2.5.3}}
\newlabel{metrics}{{2.5.3}{23}{Output Metrics}{subsubsection.2.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Evaluation and Submission}{24}{subsection.2.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Evaluation}{24}{subsubsection.2.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Requirements}{24}{subsubsection.2.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Hints}{24}{subsubsection.2.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Autolab Submission}{25}{subsubsection.2.6.4}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Implementation Details for Logistic Regression}{26}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Examples of Features}{26}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Efficient Computation of the Dot-Product}{26}{subsection.A.2}}
\newlabel{efficientdp}{{A.2}{26}{Efficient Computation of the Dot-Product}{subsection.A.2}{}}
\newlabel{eq:fastdot}{{A.2}{26}{Efficient Computation of the Dot-Product}{equation.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Data Structures for Fast Dot-Product}{27}{subsection.A.3}}
\newlabel{datastructuredp}{{A.3}{27}{Data Structures for Fast Dot-Product}{subsection.A.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Note for out-of-vocabulary features}{27}{subsection.A.3}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Abstract representation of the input file format. The $i$th row of this file will be used to construct the $i$th training example using either Model 1 features (Table \ref  {tab:model1sparse}) or Model 2 features (Table \ref  {tab:model2sparse}).\relax }}{28}{table.caption.4}}
\newlabel{tab:inputfile}{{A.1}{28}{Abstract representation of the input file format. The $i$th row of this file will be used to construct the $i$th training example using either Model 1 features (Table \ref {tab:model1sparse}) or Model 2 features (Table \ref {tab:model2sparse}).\relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Dense feature representation for Model 1 corresponding to the input file in Table \ref  {tab:inputfile}. The $i$th row corresponds to the $i$th training example. Each dense feature has the size of the vocabulary in the dictionary. Punctuations are excluded.\relax }}{28}{table.caption.5}}
\newlabel{tab:model1dense}{{A.2}{28}{Dense feature representation for Model 1 corresponding to the input file in Table \ref {tab:inputfile}. The $i$th row corresponds to the $i$th training example. Each dense feature has the size of the vocabulary in the dictionary. Punctuations are excluded.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Sparse feature representation (bag-of-word representation) for Model 1 corresponding to the input file in Table \ref  {tab:inputfile}.\relax }}{28}{table.caption.6}}
\newlabel{tab:model1sparse}{{A.3}{28}{Sparse feature representation (bag-of-word representation) for Model 1 corresponding to the input file in Table \ref {tab:inputfile}.\relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Count of word representation for Model 2 corresponding to the input file in Table \ref  {tab:inputfile}. \relax }}{28}{table.caption.7}}
\newlabel{tab:countofword}{{A.4}{28}{Count of word representation for Model 2 corresponding to the input file in Table \ref {tab:inputfile}. \relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces Sparse feature representation for Model 2 corresponding to the input file in Table \ref  {tab:inputfile}. Assume that the trimming threshold is 4. As a result, "dog" in example 2 and "apple" in example 3 are removed and the value of all remaining features are reset to value 1.\relax }}{29}{table.caption.8}}
\newlabel{tab:model2sparse}{{A.5}{29}{Sparse feature representation for Model 2 corresponding to the input file in Table \ref {tab:inputfile}. Assume that the trimming threshold is 4. As a result, "dog" in example 2 and "apple" in example 3 are removed and the value of all remaining features are reset to value 1.\relax }{table.caption.8}{}}
\newlabel{LastPage}{{}{29}{}{page.29}{}}
\xdef\lastpage@lastpage{29}
\xdef\lastpage@lastpageHy{29}
